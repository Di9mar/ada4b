{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Di9mar/ada4b/blob/main/text%20classification%20initial%20run.ipynb","timestamp":1705172670593}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Source:\n","# https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro/viewer/default/train"],"metadata":{"id":"p62pEt3MrS2E","executionInfo":{"status":"ok","timestamp":1705172945102,"user_tz":-60,"elapsed":3,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yOmT1ILVXddw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"416cb891-9617-451c-81d5-fef971b5fea9","executionInfo":{"status":"ok","timestamp":1705173012267,"user_tz":-60,"elapsed":67167,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Collecting pip\n","  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.1.2\n","    Uninstalling pip-23.1.2:\n","      Successfully uninstalled pip-23.1.2\n","Successfully installed pip-23.3.2\n","Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Collecting accelerate\n","  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill, multiprocess, accelerate, datasets\n","Successfully installed accelerate-0.26.1 datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting transformers[torch]\n","  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.26.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed transformers-4.36.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# Upgrade pip\n","!pip install --upgrade pip\n","\n","# Install required packages\n","!pip install datasets transformers torch scikit-learn accelerate\n","\n","# If you specifically need the 'torch' extras from transformers\n","!pip install transformers[torch] --upgrade"]},{"cell_type":"code","source":["from datasets import load_dataset\n","import pandas as pd\n","import torch\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, DistilBertConfig, DistilBertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score\n","import os"],"metadata":{"id":"JB_v3qlOXtGp","executionInfo":{"status":"ok","timestamp":1705173412594,"user_tz":-60,"elapsed":13523,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZArTlG_YD6s","outputId":"ba3a68d8-9c14-4f30-cdbf-d1d4380ae9f1","executionInfo":{"status":"ok","timestamp":1705173416668,"user_tz":-60,"elapsed":1697,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Define paths\n","base_path = \"/content/drive/My Drive/1-1i01FVxECPT9vpfu6Ye5nEpz3qw1jnO/MyModel\"\n","checkpoint_path = f\"{base_path}/checkpoints\"\n","trained_model_path = f\"{base_path}/trained_model\"\n","logs_path = f\"{base_path}/logs\"\n","csv_path = f\"{base_path}/wiki_data.csv\"\n","\n","# Create directories if they don't exist\n","os.makedirs(base_path, exist_ok=True)\n","os.makedirs(checkpoint_path, exist_ok=True)\n","os.makedirs(trained_model_path, exist_ok=True)\n","os.makedirs(logs_path, exist_ok=True)"],"metadata":{"id":"WhLTxdT1YEFi","executionInfo":{"status":"ok","timestamp":1705173420985,"user_tz":-60,"elapsed":376,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load data from directory on huggingface.co\n","dataset = load_dataset(\"aadityaubhat/GPT-wiki-intro\")\n","\n","# Combine all splits into one DataFrame\n","df = pd.concat([dataset[split].to_pandas() for split in dataset.keys()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXPFc74fXi3L","outputId":"7270d935-6862-4122-ed53-4f60777e8f77","executionInfo":{"status":"ok","timestamp":1705173430318,"user_tz":-60,"elapsed":6177,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Save the DataFrame as a CSV file in the specified directory on Google Drive\n","df.to_csv(csv_path, index=False)\n","\n","# Load the data into a Pandas DataFrame\n","df = pd.read_csv(csv_path)"],"metadata":{"id":"GKVbIKgqXi5E","executionInfo":{"status":"ok","timestamp":1705173461287,"user_tz":-60,"elapsed":22019,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Separate 10% of the data as test data\n","train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n","\n","# Splitting the training DataFrame into 3 equal parts\n","split_size = len(train_df) // 3\n","df1, df2, df3 = train_df.iloc[:split_size], train_df.iloc[split_size:2*split_size], train_df.iloc[2*split_size:]"],"metadata":{"id":"6WjR7CySBXcG","executionInfo":{"status":"ok","timestamp":1705173468444,"user_tz":-60,"elapsed":214,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Prepare the texts and labels for the test data\n","test_texts = test_df['wiki_intro'].tolist() + test_df['generated_intro'].tolist()\n","test_labels = [0] * len(test_df['wiki_intro']) + [1] * len(test_df['generated_intro'])\n","\n","# For df1\n","train_labels_df1 = [0] * len(df1['wiki_intro']) + [1] * len(df1['generated_intro'])\n","train_texts_df1 = df1['wiki_intro'].tolist() + df1['generated_intro'].tolist()\n","\n","# For df2\n","train_labels_df2 = [0] * len(df2['wiki_intro']) + [1] * len(df2['generated_intro'])\n","train_texts_df2 = df2['wiki_intro'].tolist() + df2['generated_intro'].tolist()\n","\n","# For df3\n","train_labels_df3 = [0] * len(df3['wiki_intro']) + [1] * len(df3['generated_intro'])\n","train_texts_df3 = df3['wiki_intro'].tolist() + df3['generated_intro'].tolist()\n"],"metadata":{"id":"6CF7NFTqGzS9","executionInfo":{"status":"ok","timestamp":1705173471402,"user_tz":-60,"elapsed":274,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Use a small, fast model for quick training (DistilBERT)\n","model_name = \"distilbert-base-uncased\"\n","config = DistilBertConfig.from_pretrained(model_name)\n","config.num_labels = 2\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWZwCNhwXi90","outputId":"3fe007a2-4ea2-4b6e-fa34-1657c4c8c2e0","executionInfo":{"status":"ok","timestamp":1705173474890,"user_tz":-60,"elapsed":1756,"user":{"displayName":"Benedict Bruhn","userId":"10891844952633909514"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Define dataset class\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts, labels, tokenizer):\n","        self.encodings = tokenizer(texts, truncation=True, padding=True)\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Preparing train datasets for each subset\n","train_dataset_df1 = TextDataset(train_texts_df1, train_labels_df1, tokenizer)\n","train_dataset_df2 = TextDataset(train_texts_df2, train_labels_df2, tokenizer)\n","train_dataset_df3 = TextDataset(train_texts_df3, train_labels_df3, tokenizer)\n","\n","# Preparing the validation dataset (assuming you have separate validation data)\n","val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n","\n","# Preparing the test dataset\n","test_dataset = TextDataset(test_texts, test_labels, tokenizer)"],"metadata":{"id":"yIil8FW9XjAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=checkpoint_path,\n","    num_train_epochs=1,  # We'll manually loop over epochs\n","    per_device_train_batch_size=16,  # Adjust based on your GPU memory\n","    gradient_accumulation_steps=2,  # Increase if using a larger effective batch size\n","    evaluation_strategy=\"epoch\",\n","    logging_dir=logs_path,\n","    logging_steps=50,\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"],"metadata":{"id":"q8UWAmlrkX7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to initialize the Trainer with a specific training dataset\n","def initialize_trainer_for_subset(train_dataset, eval_dataset, model, training_args):\n","    return Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","    )\n","\n","# Define the number of manual epochs\n","num_manual_epochs = 3  # Set the number of manual epochs\n","\n","# Looping over the manual epochs\n","for epoch in range(num_manual_epochs):\n","    print(f\"Starting manual epoch {epoch + 1}/{num_manual_epochs}\")\n","\n","    # Iterate over each training subset within each manual epoch\n","    for subset_index, train_dataset in enumerate([train_dataset_df1, train_dataset_df2, train_dataset_df3]):\n","        print(f\"Training on subset {subset_index + 1}\")\n","\n","        # Initialize the Trainer with the current training subset\n","        trainer = initialize_trainer_for_subset(train_dataset, val_dataset, model, training_args)\n","\n","        # Train the model for one epoch on the current subset\n","        trainer.train()\n","\n","        # Optionally, evaluate the model after each subset\n","        # results = trainer.evaluate()\n","\n","    # After completing all subsets for the manual epoch, you can save the model\n","    # print(f\"Saving model after manual epoch {epoch + 1}\")\n","    # trainer.save_model(f\"{trained_model_path}/manual_epoch_{epoch + 1}\")\n","    # tokenizer.save_pretrained(f\"{trained_model_path}/manual_epoch_{epoch + 1}\")"],"metadata":{"id":"IpdEfOpeXjCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluatoin / Testing\n","\n","# Create the test dataset\n","test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n","\n","# Function to initialize the Trainer for evaluation\n","def initialize_trainer_for_evaluation(model, training_args, eval_dataset):\n","    return Trainer(\n","        model=model,\n","        args=training_args,\n","        eval_dataset=eval_dataset,\n","    )\n","\n","# Initialize the Trainer for evaluation with the test dataset\n","eval_trainer = initialize_trainer_for_evaluation(model, training_args, test_dataset)\n","\n","# Evaluate the model on the test set\n","results = eval_trainer.evaluate()\n","\n","print(\"Training completed successfully.\")\n","print(\"Evaluation results:\", results)"],"metadata":{"id":"gSCuFEkN5Bw4"},"execution_count":null,"outputs":[]}]}