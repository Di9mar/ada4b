{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNRjif9mWxIAVpZ+QSrbUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Di9mar/ada4b/blob/main/Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pop4rjwSvvyJ",
        "outputId": "8e59c6c5-c33d-4bed-9756-e561263a610f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: dill, multiprocess, accelerate, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.26.1 datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 transformers-4.36.2\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install datasets transformers[torch] --upgrade\n",
        "\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'drive' module from the 'google.colab' library\n",
        "# This module allows you to mount your Google Drive in the Colab environment.\n",
        "# Make sure you have the necessary authorization to access your Drive.\n",
        "# If not already installed, you may need to install the 'google-colab' package.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to '/content/drive'\n",
        "# This will make your Google Drive files accessible from within the Colab environment.\n",
        "# You'll be prompted to authenticate and grant necessary permissions.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FkubkSuv_n1",
        "outputId": "1a818037-b46c-4e64-9938-902cbb156745"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "current_model = \"wiki\"\n",
        "data_file = \"story_data\"\n",
        "\n",
        "# Define paths based on your original code\n",
        "base_path = \"/content/drive/My Drive/ColabData\"\n",
        "model_path = f\"{base_path}/{current_model}\"\n",
        "csv_path = f\"{base_path}/{data_file}.csv\"\n",
        "logs_path = f\"{base_path}/logs\""
      ],
      "metadata": {
        "id": "C5UyUEJwv_pv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model and tokenizer\n",
        "print(f\"Loading '{current_model}' model\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# Print the model configuration for reference\n",
        "print(f\"Model Configuration:\\n{model.config}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIPieZ4Sv_sK",
        "outputId": "bfd3d451-7a1e-4a7b-9a26-c7935d895bbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'wiki' model\n",
            "Model Configuration:\n",
            "DistilBertConfig {\n",
            "  \"_name_or_path\": \"/content/drive/My Drive/ColabData/wiki\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.36.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # Tokenize the text on-the-fly\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # Convert the encoding to a format suitable for PyTorch\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}  # Squeeze is used to remove batch dimension\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.labels\n",
        "\n",
        "\n",
        "# Function to Calculate Metrics\n",
        "def calculate_evaluation_metrics(predictions, true_labels):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
        "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "    roc_auc = roc_auc_score(true_labels, predictions)  # For binary classification\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'F1_score': f1,\n",
        "        'confusion_matrix': conf_matrix.tolist(),\n",
        "        'ROC_AUC': roc_auc\n",
        "    }"
      ],
      "metadata": {
        "id": "p_5Tg6u1v_un"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load new data\n",
        "try:\n",
        "    df = pd.read_csv(csv_path, delimiter=';')\n",
        "    load_success = True\n",
        "except Exception as e:\n",
        "    load_success = False\n",
        "    df = None\n",
        "    error_message = str(e)\n",
        "\n",
        "load_success, df if df is not None else error_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E4PgWgTv_wu",
        "outputId": "748a172d-3f5a-462a-d585-aa5b6ca182eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True,\n",
              "                                                  human  \\\n",
              " 0    Chapter Text\\n\\n\\nThey’d just fired him.\\n\\n\\n...   \n",
              " 1    Stu wakes up in a coffin and thinks, fuck. Las...   \n",
              " 2    It was the holiday season in Bound Arlyn, and ...   \n",
              " 3    His eyes were so warm and intense on me that I...   \n",
              " 4    Ada: Salazar Castle: Bedroom:\\n\\n \\nI sat perc...   \n",
              " ..                                                 ...   \n",
              " 173  A letter laid in his hand. A date not even rea...   \n",
              " 174  Chapter Text\\n\\nDistrict One: Female- Valentin...   \n",
              " 175  Intak always keep an eye on Jiung. How he sudd...   \n",
              " 176  Once upon a time, in a small mountain town cal...   \n",
              " 177  “I don’t need you anymore.”\\n\\n\\n\\n I repeat t...   \n",
              " \n",
              "                                                     ai  \n",
              " 0    In a world where soulmates actually exists, wh...  \n",
              " 1    So i suddenly came up with this idea, what if ...  \n",
              " 2    Once upon a time it was an average day in the ...  \n",
              " 3    Bella sat there waiting, watching the clock. T...  \n",
              " 4    Smithers sat in the dimly-lit interrogation ro...  \n",
              " ..                                                 ...  \n",
              " 173  \\nI was born with the ability to transform int...  \n",
              " 174  \\nOnce upon a time, there was a young woman na...  \n",
              " 175  Magic and technology have coexisted for centur...  \n",
              " 176  The day started out like any other for 16-year...  \n",
              " 177  The world is a very different place than it wa...  \n",
              " \n",
              " [178 rows x 2 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove excess newline characters\n",
        "df['human'] = df['human'].str.replace(r'\\n+', '\\n')\n",
        "df['ai'] = df['ai'].str.replace(r'\\n+', '\\n')\n",
        "\n",
        "# Prepare the data\n",
        "labels = [0] * len(df['human']) + [1] * len(df['ai'])  # Adjust columns as per your data\n",
        "texts = df['human'].tolist() + df['ai'].tolist()  # Adjust columns as per your data\n",
        "dataset = TextDataset(texts, labels, tokenizer)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "o1Vf8BmSv_zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c33ecf-f01d-495a-9bb9-d736aea4ce13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-42ff40dda06b>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['human'] = df['human'].str.replace(r'\\n+', '\\n')\n",
            "<ipython-input-11-42ff40dda06b>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['ai'] = df['ai'].str.replace(r'\\n+', '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "successful_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        # Tokenize the text\n",
        "        encoding = tokenizer(row['human'], row['ai'], truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "        successful_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        error_count += 1\n",
        "        # Print the error message and the problematic texts\n",
        "        print(f\"Error in row {idx}: {str(e)}\")\n",
        "        print(f\"Problematic 'human' text (row {idx}):\\n{row['human']}\\n\")\n",
        "        print(f\"Problematic 'ai' text (row {idx}):\\n{row['ai']}\\n\")\n",
        "\n",
        "# Print the summary at the end\n",
        "print(f\"Total rows processed: {successful_count + error_count}\")\n",
        "print(f\"Successful tokenizations: {successful_count}\")\n",
        "print(f\"Tokenization errors: {error_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeGPVkZEUEDX",
        "outputId": "6d2f1c4b-202c-40bf-ec62-ae5e6fe747e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows processed: 178\n",
            "Successful tokenizations: 178\n",
            "Tokenization errors: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the new dataset\n",
        "predictions = trainer.predict(dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = calculate_evaluation_metrics(predicted_labels, dataset.get_labels())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5-HxJWIWv_1I",
        "outputId": "b8fdb900-5ff2-4ccc-e31b-ab508bc2acdb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the final evaluation metrics\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", metrics['accuracy'])\n",
        "print(\"Precision:\", metrics['precision'])\n",
        "print(\"Recall:\", metrics['recall'])\n",
        "print(\"F1 Score:\", metrics['F1_score'])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(metrics['confusion_matrix'])\n",
        "print(\"ROC AUC:\", metrics['ROC_AUC'])"
      ],
      "metadata": {
        "id": "eiESIHY8v_3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d29364-33b8-4e5d-b63e-ab6a453778e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Metrics:\n",
            "Accuracy: 0.7696629213483146\n",
            "Precision: 0.9137931034482759\n",
            "Recall: 0.5955056179775281\n",
            "F1 Score: 0.7210884353741497\n",
            "Confusion Matrix:\n",
            "[[168, 10], [72, 106]]\n",
            "ROC AUC: 0.7696629213483145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file name for the metrics using current_model and data_file\n",
        "metrics_file = f\"{logs_path}/evaluation_metrics_{current_model}_{os.path.basename(data_file).split('.')[0]}.json\"\n",
        "\n",
        "# Save the evaluation metrics\n",
        "try:\n",
        "    with open(metrics_file, 'w') as file:\n",
        "        json.dump(metrics, file, indent=4)\n",
        "    print(f\"Evaluation metrics saved in {metrics_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the metrics: {str(e)}\")"
      ],
      "metadata": {
        "id": "WiPFfbHJv_5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6724e09-d2cd-4619-9b0f-419fc053239f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics saved in /content/drive/My Drive/ColabData/logs/evaluation_metrics_wiki_story_data.json\n"
          ]
        }
      ]
    }
  ]
}